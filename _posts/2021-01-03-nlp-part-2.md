---
layout: single
title:  "NLP "
header:
  teaser: /assets/images/thumbnails/NLP_image.jpg
excerpt: "Word Embeddings"
date:   2021-01-03
category: nlp
tags: [nlp, cs224n, word2vec, ]
comments: true
sidebar:
  nav: "docs"
---

Today we are going to talk about word embeddings. This is the second part of the NLP series. See summary and other posts here.

### Intro

In the previous post we mentioned that since 2013, people started to use neural nets style representation like word2vec, etc.

### Word Embedding

### Count-Based model and Co-occurance matrix
[this paper] (http://www.cnbc.cmu.edu/~plaut/papers/pdf/RohdeGonnermanPlautSUB-CogSci.COALS.pdf)


#### Word2Vec

2013

Related paper and note:

- [Distributed Representations of Words and Phrases
and their Compositionality](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)

- [Efficient Estimation of Word Representations in Vector Space]()

- [word2vec Explained](https://arxiv.org/pdf/1402.3722.pdf). 

This paper served as a summary/note to further explain the word2vec model in the previous paper. Specifically on negative sampling (drawn by unigram distribution to the power of 3/4) based on skip-gram model, and its ojective function. Note that the words and contexts representation are learned jointly so the model is non-convex(comparing to learning only the word representation while fixing contexts, it will reduced to a logistic regression which is convex).

Here is a interesting Q&A quoted from the paper:

Why does this produce good word representations?

Good question. We don’t really know.
The distributional hypothesis states that words in similar contexts have similar meanings. The objective above clearly tries to increase the quantity $v_{w} \dot v_{c}$ for good word-context pairs, and decrease it for bad ones. Intuitively, this
means that words that share many contexts will be similar to each other (note also that contexts sharing many words will also be similar to each other). This is, however, very hand-wavy.
Can we make this intuition more precise? We’d really like to see something
more formal.





### Context-Based model

![](/assets/images/post_image/CBOW-and-Skip-Gram.jpg)

#### Skip-Gram

![](/assets/images/post_image/word2vec_skipgram.png)

#### Continuous Bag-of-words

- [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf) this summary paper helped explain the detailed of CBOW and Skip-Gram models and herachical softmax and negative sampling.

- hw1 on co-occurance [here](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a1_preview/exploring_word_vectors.html)

- hw2 on implementing word2vec [here](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/assignments/a2.pdf)


#### GloVe
Global Vector (GloVe) model tries to combine count-based Matrix Factorization and skip-gram model.







## Reference


- CS229 winter 2019 [course website ](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

- [Google's word2vec project archive](https://code.google.com/archive/p/word2vec/sa) 
- [Deep Learning for Natural Language Processing: Tutorials with Jupyter Notebooks](https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f)

- [TensorFlow tutorials] (https://www.tensorflow.org/tutorials/text/word2vec)
- blog post [Learning Word Embedding] (https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) by Lilian Weng


- Up Next

I will post some learning notes about NLP. Stay tuned!





