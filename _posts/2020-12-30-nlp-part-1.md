---
layout: single
title:  "Intro to NLP Part I: Resource & Summary"
header:
  teaser: /assets/images/thumbnails/NLP_image.jpg
excerpt: "Learning NLP: Some Resources and summary of recent models."
date:   2020-12-30
category: nlp
tags: [nlp, course, word-embedding, cs224n, word2vec, language-model, bert]
comments: true
sidebar:
  nav: "docs"
---




Today we are going to talk about NLP! I have listed some resources that are helpful to start the learning journey in this field. I also did a summary of some of the recent topics and models in NLP. This is the firsts part of the NLP series. See other posts [here](https://jiajingchen.github.io/categories/#nlp).




## Resource

### Courses:


The first three are graduate level NLP courses with focus on neural nets applications and state-of-the-art models (e.g. Attention, Transformer, BERT, Multi-lingual, etc) while the last two courses are more introductory courses.

- [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z) Natural Language Processing with Deep Learning by Christopher Mannings

- [CMU CS11-737](https://www.youtube.com/watch?v=xeu7LKIT194&list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5) Multilingual NLP 2020, by Graham Neubig
- [CMU CS11-737](https://www.youtube.com/watch?v=D7o2Z1tAuQc&list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ) Neural Nets for NLP 2020 by Graham Neubig

- [Stanford CS224U](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20) Natural Language Understanding

- [Stanford CS124](https://www.youtube.com/channel/UC_48v322owNVtORXuMeRmpA) From Languages to Information by Dan Jurafsky


### Textbook:

- [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) by Jacob Eisenstein

- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) by Dan Jurafsky and James H. Martin

- [A Primer on Neural Network Models
for Natural Language Processing](https://u.cs.biu.ac.il/~yogo/nnlp.pdf) by Yoav Goldberg

- [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/information-retrieval-book.html) by Christopher Manning, Prabhakar Raghavan and Hinrich Sch√ºtze

## Summary



Chris mentioned in CS224N that NLP can be categorized into more tranditional NLP (up until approximately 2012) and more "neural nets stype" representation and models since 2013. Today we are going to focus on some of the recent breakthroughs in NLP.

### Word Embedding

Word embedding is a dense representation of words in a vector space. The biggest jump in NLP when moving from the sparse-input linear models to neural-network based models is to represent each feature as a dense vectors rather than each feature as a unique dimension (one-hot encoding). One of the advantages of word embeddings over one-hot encoding is the ability to generalize: semantically similar words are similar in vector representation, thus are close to each other in the vector space. Additionally, it can reveal the hidden semantic relationship between words by their relative positions in the vector space. One notable example is that v("king")-v("man")+v("woman") should be close to v("queen").


There are two main approaches to learn the word enbeddings, count-based and context-based. Count-based methods utilize word frequency count and co-occurance matrix. The main idea is that words in the same contexts are more likely to be similar in semantic meanings. Context-based methods converted the problem into a predictive model that predict the central word given context words (Continuous Bag-of-Words, CBOW) or predict a word being a context word for the given target (also known as Skip-gram mpdel).

Word2Vec ([Mikolov et al., 2013](https://arxiv.org/pdf/1301.3781.pdf)) is a popular technique to learn word embeddings using a two-layer neural net. It was introduced by a team of researchers at Google led by Tomas Mikolov. Global Vector(GloVe) is another model that combines count-based Matrix Factorization and skip-gram model. We will talk about the details of Word2Vec and GloVe in our next [post](https://jiajingchen.github.io/nlp/nlp-part-2/). 


### Language Models
Language models are models that assign probability to sequence of words. N-gram is a simple model that assign sequance of n words. For example "my book", "watch out" is a 2-gram (bigram), "this is a" is a 3-gram (trigram), etc. N-gram model will assign/estimate the probability of the n+1 word given the previous n words. 

There are more complicated language model like RNN LMs, Seq2Seq, and more recently, the pre-trained language models like BERT, GPT-3, etc.


- ELMo
- BERT
- T5
- GPT-3
- Attention mechanism



<i class="far fa-sticky-note"></i> **Up Next:** 
We will talk about word embeddings and the state-of-the-art language models in more details in our next post. Stay tuned!
{: .notice--info}
{: .text-justify}



## Reference


- [CS224N, winter 2019](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

- [Deep Learning for Natural Language Processing: Tutorials with Jupyter Notebooks](https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f)

- [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) by Lilian Weng




