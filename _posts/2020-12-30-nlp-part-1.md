---
layout: single
title:  "Intro to NLP Part I: Resource & Summary"
header:
  teaser: /assets/images/thumbnails/NLP_image.jpg
excerpt: "Learning NLP: Some Resources and summary of recent models."
date:   2020-12-30
category: nlp
tags: [nlp, course, word-embedding, cs224n, word2vec, language-model, bert]
comments: true
sidebar:
  nav: "docs"
---




Today we are going to talk about NLP! I have listed some resources that are helpful to start the learning journey in this field. I also did a summary of some of the recent topics and models in NLP. This is the firsts part of the NLP series. See other posts [here](https://jiajingchen.github.io/categories/#nlp).




## Resource

### Courses:


The first three are graduate level NLP courses with focus on neural nets applications and state-of-the-art models (e.g. Attention, Transformer, BERT, Multi-lingual, etc) while the last two courses are more introductory courses.

- [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z) Natural Language Processing with Deep Learning by Christopher Mannings

- [CMU CS11-737](https://www.youtube.com/watch?v=xeu7LKIT194&list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5) Multilingual NLP 2020, by Graham Neubig
- [CMU CS11-737](https://www.youtube.com/watch?v=D7o2Z1tAuQc&list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ) Neural Nets for NLP 2020 by Graham Neubig

- [Stanford CS224U](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20) Natural Language Understanding

- [Stanford CS124](https://www.youtube.com/channel/UC_48v322owNVtORXuMeRmpA) From Languages to Information by Dan Jurafsky


### Textbook:

- [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) by Jacob Eisenstein

- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) by Dan Jurafsky and James H. Martin

- [A Primer on Neural Network Models
for Natural Language Processing](https://u.cs.biu.ac.il/~yogo/nnlp.pdf) by Yoav Goldberg

- [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/information-retrieval-book.html) by Christopher Manning, Prabhakar Raghavan and Hinrich Sch√ºtze

## Summary



Chris mentioned in CS224N that NLP can be categorized into more tranditional NLP (up until approximately 2012) and more "neural nets stype" representation and models since 2013. Today we are going to focus on some of the recent breakthroughs in NLP.

### Word Embedding

Word embedding is a dense representation of words in a vector space. The biggest jump in NLP when moving from the sparse-input linear models to neural-network based models is to represent each feature as a dense vectors rather than each feature as a unique dimension (one-hot encoding/representation).


There are two main approaches to learn the word enbeddings, count-based and context-based. 
 
- Count-Based model 

Count-based methods utilize word frequency count and co-occurance matrix (similar to the co-occurance matrix in recommendation system). The idea is that words in the same contexts are more likely to be similar in semantic meanings.

- Context-Based model

Context-based methods converted the problem into a predictive model (similar to predictive approach in recommendation system) that predict the central word given context words. While training the models, the word representation is learned as the model parameters. Some of the context-based models are Skip-Gram model, Continuous Bag-of-Words (CBOW) model. 


In general, the word embeddings can be learned during the training process (or alternatively we can use pre-trained models instead). For most methods, it's like training other parameters in the neural network. The word embeddings is the hidden layer that is optimized by predicting a target word given context words (CBOW), or predicting a word being a context word for the given target (Skip-gram model, used in Word2vec).

- Word2Vec

Word2Vec ([Mikolov et al., 2013](https://arxiv.org/pdf/1301.3781.pdf)) is a popular technique to learn word embeddings using a two-layer neural net. It was introduced by a team of researchers at Google led by Tomas Mikolov. Google hosted a open source version of the Word2Vec model [here](https://code.google.com/archive/p/word2vec/). 



We will talk about word embeddings in more details in the next post here.



### Language Models
Language models are models that assign probability to sequence of words. N-gram is a simple model that assign sequance of n words. For example "my book", "watch out" is a 2-gram (bigram), "this is a" is a 3-gram (trigram), etc. N-gram model will assign/estimate the probability of the n+1 word given the previous n words. 

There are more complicated language model like RNN LMs, Seq2Seq, and more recently, the pre-trained language models like BERT, GPT-3, etc.


- ElMO
- BERT
- T5
- GPT-3
- Attention mechanism



<i class="far fa-sticky-note"></i> **Up Next:** 
We will talk about word embeddings and the state-of-the-art language models in more details in our next post. Stay tuned!
{: .notice--info}
{: .text-justify}



## Reference


- [CS224N, winter 2019](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

- [Deep Learning for Natural Language Processing: Tutorials with Jupyter Notebooks](https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f)

- [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) by Lilian Weng




