---
layout: single
title:  "Attention"
header:
  teaser: /assets/images/thumbnails/NLP_image.jpg
excerpt: "Attention please!"
date:   2021-01-13
category: nlp
tags: [nlp, cs224n, word2vec, attention, transformer, bert]
comments: true
mathjax: true
toc: true
toc_label: "Intro to NLP: Attention"
toc_icon: align-left
toc_sticky: true
sidebar:
  nav: "docs"
---

Today we are going to talk about Attention. This is part of the NLP series. See summary and other posts here.

### BERT
In the previous post we mentioned that since 2013, people started to use neural nets style representation like word2vec, etc.

### Attention

video 224N starting from [here](https://youtu.be/XXtpJxZBa2c?t=3653)


### Reference


- CS229 winter 2019 [course website ](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

Lecture 8


- blog post [Learning Word Embedding] (https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) by Lilian Weng



- Up Next

I will post more learning notes about NLP. Stay tuned!





