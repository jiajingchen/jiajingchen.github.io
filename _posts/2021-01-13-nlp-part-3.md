---
layout: single
title:  "Attention"
header:
  teaser: /assets/images/thumbnails/NLP_image.jpg
excerpt: "Attention please!"
date:   2021-01-13
category: nlp
tags: [nlp, cs224n, attention, transformer, bert, neural-net, language-model]
comments: true
mathjax: true
toc: true
toc_label: "Intro to NLP: Attention"
toc_icon: align-left
toc_sticky: true
sidebar:
  nav: "docs"
---

Today we are going to talk about Attention. This is third part of the NLP series. See summary and other posts [here](https://jiajingchen.github.io/categories/#nlp).

### Transformer & BERT


### Attention
Transformer based architectures are built around idea of Attention.
In the paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762)

### Reference


- CS229 winter 2019 [course website ](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/), Lecture ()


- [Attention is All You Need](https://arxiv.org/abs/1706.03762)

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention)


- blog post [Learning Word Embedding] (https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) by Lilian Weng



- Up Next

I will post more learning notes about NLP. Stay tuned!





